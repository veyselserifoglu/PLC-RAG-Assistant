{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d886fa9",
   "metadata": {},
   "source": [
    "# Embedding Model Comparison for PLC Data\n",
    "\n",
    "**Objective:** To identify an open-source embedding model that effectively captures the semantic meaning of PLC documentation and code snippets for relevant retrieval.\n",
    "\n",
    "**Methodology:**\n",
    "1.  **Load Data:** Prepare a representative dataset of PLC documents and code snippets.\n",
    "2.  **Select Models:** Choose a few candidate sentence-transformer models.\n",
    "3.  **Embed & Query:** For each model, embed the dataset and a list of test queries.\n",
    "4.  **Retrieve & Evaluate:** Perform similarity search, retrieve top K chunks, and qualitatively evaluate relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88feb488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Import Libraries\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss # For FAISS, if used. Alternatively, use sklearn.metrics.pairwise.cosine_similarity for smaller datasets\n",
    "import numpy as np\n",
    "\n",
    "# Placeholder for your data loading function\n",
    "def load_plc_data():\n",
    "    \"\"\"\n",
    "    Loads PLC documents and code snippets.\n",
    "    Replace this with your actual data loading logic.\n",
    "    Should return a list of strings, where each string is a document or code chunk.\n",
    "    \"\"\"\n",
    "    # Example:\n",
    "    documents = [\n",
    "        \"CODESYS Function Block Diagram (FBD) is a graphical programming language.\",\n",
    "        \"Structured Text (ST) in CODESYS resembles Pascal or C.\",\n",
    "        \"The TON timer function block provides an on-delay timing.\",\n",
    "        \"VAR_INPUT TempSensor : REAL; END_VAR\",\n",
    "        \"IF TempSensor > 100 THEN Alarm := TRUE; END_IF;\"\n",
    "        # Add more representative PLC documents and code snippets here\n",
    "    ]\n",
    "    return documents\n",
    "\n",
    "# Placeholder for your test queries\n",
    "def get_test_queries():\n",
    "    \"\"\"\n",
    "    Returns a list of test queries relevant to PLC programming.\n",
    "    \"\"\"\n",
    "    queries = [\n",
    "        \"How to use a TON timer in CODESYS?\",\n",
    "        \"What is Structured Text syntax for IF statements?\",\n",
    "        \"Explain Function Block Diagrams.\",\n",
    "        \"Example of variable declaration in CODESYS.\"\n",
    "        # Add more realistic queries\n",
    "    ]\n",
    "    return queries\n",
    "\n",
    "plc_documents = load_plc_data()\n",
    "test_queries = get_test_queries()\n",
    "\n",
    "print(f\"Loaded {len(plc_documents)} documents.\")\n",
    "print(f\"Loaded {len(test_queries)} test queries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5daab581",
   "metadata": {},
   "source": [
    "## 2. Define Embedding Models to Compare\n",
    "List the Sentence Transformer model names you want to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f1f4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model_names = [\n",
    "    \"all-MiniLM-L6-v2\",       # A good general-purpose lightweight model\n",
    "    \"BAAI/bge-small-en-v1.5\", # Another strong contender, good balance\n",
    "    \"sentence-transformers/multi-qa-mpnet-base-dot-v1\" # Tuned for QA tasks\n",
    "    # Add other models you want to test, e.g., multilingual if needed\n",
    "]\n",
    "\n",
    "models = {name: SentenceTransformer(name) for name in embedding_model_names}\n",
    "print(f\"Loaded models: {list(models.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7269f17b",
   "metadata": {},
   "source": [
    "## 3. Embed Documents and Queries & Perform Retrieval\n",
    "\n",
    "For each model:\n",
    "- Embed the PLC documents.\n",
    "- Embed the test queries.\n",
    "- For each query, find the top K most similar documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8da81f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_k_similar(query_embedding, document_embeddings, documents, k=3):\n",
    "    # Using FAISS for efficient search (can be replaced with simple cosine similarity for small datasets)\n",
    "    dimension = document_embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension) # L2 distance works well for normalized embeddings\n",
    "    index.add(document_embeddings.astype(np.float32))\n",
    "    \n",
    "    distances, indices = index.search(query_embedding.astype(np.float32).reshape(1, -1), k)\n",
    "    \n",
    "    return [(documents[i], 1 - d) for i, d in zip(indices[0], distances[0])] # Convert L2 dist to a pseudo-similarity\n",
    "\n",
    "results = {} # To store results for each model\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\n--- Processing with model: {model_name} ---\")\n",
    "    model_results = []\n",
    "    \n",
    "    # Embed documents\n",
    "    doc_embeddings = model.encode(plc_documents, convert_to_tensor=False, show_progress_bar=True)\n",
    "    \n",
    "    for query in test_queries:\n",
    "        query_embedding = model.encode(query, convert_to_tensor=False)\n",
    "        \n",
    "        # Get top K similar documents\n",
    "        # For simplicity, using basic cosine similarity here if FAISS is not preferred for small scale\n",
    "        # from sklearn.metrics.pairwise import cosine_similarity\n",
    "        # similarities = cosine_similarity(query_embedding.reshape(1, -1), doc_embeddings)\n",
    "        # top_k_indices = np.argsort(similarities[0])[::-1][:3]\n",
    "        # top_k_docs = [(plc_documents[i], similarities[0][i]) for i in top_k_indices]\n",
    "        \n",
    "        top_k_docs = get_top_k_similar(query_embedding, doc_embeddings, plc_documents, k=3)\n",
    "        \n",
    "        model_results.append({\n",
    "            \"query\": query,\n",
    "            \"retrieved_chunks\": top_k_docs\n",
    "        })\n",
    "        \n",
    "        print(f\"\\nQuery: {query}\")\n",
    "        for doc, score in top_k_docs:\n",
    "            print(f\"  Retrieved: {doc[:100]}... (Score: {score:.4f})\")\n",
    "            \n",
    "    results[model_name] = model_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21d09a0",
   "metadata": {},
   "source": [
    "## 4. Qualitative Evaluation\n",
    "\n",
    "Review the `results` dictionary. For each model and each query, assess the relevance of the retrieved chunks.\n",
    "\n",
    "**Considerations for Evaluation:**\n",
    "- **Relevance:** How well do the retrieved chunks address the query?\n",
    "- **Specificity:** Do they provide specific information or just general context?\n",
    "- **Diversity:** If multiple chunks are retrieved, do they offer different facets of the answer or are they redundant?\n",
    "\n",
    "Based on this qualitative review, you can decide which embedding model performs best for your PLC data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3dbde90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Print results for one model to examine\n",
    "# You would typically do a more structured review, perhaps exporting to a spreadsheet or just manually going through 'results'\n",
    "if results:\n",
    "    sample_model_name = list(results.keys())[0]\n",
    "    print(f\"\\n--- Detailed Results for Model: {sample_model_name} ---\")\n",
    "    for item in results[sample_model_name]:\n",
    "        print(f\"\\nQuery: {item['query']}\")\n",
    "        for doc, score in item['retrieved_chunks']:\n",
    "            print(f\"  Retrieved: {doc} (Score: {score:.4f})\")\n",
    "else:\n",
    "    print(\"No results to display. Ensure the previous cells ran correctly.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
